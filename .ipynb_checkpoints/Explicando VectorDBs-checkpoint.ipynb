{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abcb1df3-9a2f-488d-b849-27e9f1490ea9",
   "metadata": {},
   "source": [
    "# Explicando Retrieval Augmented Generation\n",
    "\n",
    "Neste exemplo, vamos carregar um PDF com prospecto de um fundo de investimento para utilizá-lo como base de conhecimento para tratamentos de dúvidas.\n",
    "\n",
    "Utilizaremos os modelos da OpenAI para gerar embeddings e respostas.\n",
    "\n",
    "Os dados vetorizados serão armazenados no Astra/Cassandra para busca baseada na similaridade de vetores (Vector Search)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd02e3-937c-4a36-8b0c-efcbfda6bca8",
   "metadata": {},
   "source": [
    "## Configurando ambiente\n",
    "\n",
    "Inicialmente, vamos instalar e configurar o ambiente para execução dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9323a946-3fe3-4422-9d2e-335c0359be4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai cassio --upgrade "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a7ed542-8ddf-4e70-b03d-39853fa4392b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregando variáveis de ambiente\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68b4d8f-e1f2-4db6-a600-5bcc764efd08",
   "metadata": {},
   "source": [
    "## Conectando ao Astra/Cassandra\n",
    "\n",
    "Para utilizar o Cassandra como Vector DB, crie um banco de dados com suporte a vvector search em astra.datastax.com, gere um token de conexão e identifique o seu DB ID. Com isso, execute o comando abaixo para criar a conexão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b977126-a3e5-48ab-8801-87a57034e38e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ASTRA_DB_APPLICATION_TOKEN'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cb/kqbz377s0f5cnzwythftzhp40000gp/T/ipykernel_949/3923155919.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcassio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcassio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ASTRA_DB_APPLICATION_TOKEN\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ASTRA_DB_ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.9.4/lib/python3.9/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ASTRA_DB_APPLICATION_TOKEN'"
     ]
    }
   ],
   "source": [
    "import cassio\n",
    "\n",
    "cassio.init(token=os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"], database_id=os.environ[\"ASTRA_DB_ID\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8559d8-a667-4339-be3c-a683c08abce2",
   "metadata": {},
   "source": [
    "Pronto!\n",
    "\n",
    "A conexão com o Astra é muito simples e você já pode armazenar seus documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34f4f2d-98d9-4cef-8db3-c12de364ef53",
   "metadata": {},
   "source": [
    "## Criando a tabela/índice no Astra\n",
    "\n",
    "A estrutura no banco de dados que vai armazenar os dados, no caso do Astra/Cassandra, são as tabelas.\n",
    "\n",
    "O Astra possui uma modelagem baseada em partições que permite escalar os dados até terabytes e milhões de registros.\n",
    "\n",
    "Além disso, os índices possibilitam queries com suporte a vector search, como veremos a seguir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f0b72b-d2f6-4b1f-81d4-122e59bbc745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma tabela"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b15869-9eb1-496b-ad82-9ca327b8bb23",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "Vamos converter nossa base de documentos em embeddings. \n",
    "\n",
    "Se você não sabe o que são embeddings, assista aqui: https://www.youtube.com/watch?v=fvRyziDmvoA\n",
    "\n",
    "Utilizaremos o LangChain para simplificar a leitura dos PDFs, geração de chunks e posterior utilização dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e31600-2298-4ea3-9214-fbfb2885c08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neste exemplo, vou utilizar os embeddings da OpenAI.\n",
    "# Aqui, importamos o LLM e os Embeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "llm = OpenAI(temperature=0.5)\n",
    "embedding_generator = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d35541-7c84-4795-bf7c-4105ae0a4caf",
   "metadata": {},
   "source": [
    "### Definindo o Index/Tabela do Cassandra que vai armazenar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1fa8dd-399e-4e5c-9900-5d486a524eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O LangChain possui suporte ao Cassandra, então vamos utilizá-lo\n",
    "# (vamos usar o Astra, que é o Cassandra como serviço)\n",
    "# Criando o índice no Astra\n",
    "\n",
    "from langchain.vectorstores.cassandra import Cassandra\n",
    "table_name = 'vs_investment'\n",
    "keyspace = 'demo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd17881-591d-413f-a32b-071e9a7e90c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se necessário, execute este comando para limpar a tabela com os dados\n",
    "#CassVectorStore.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e5b925-d9ef-475c-b00f-646f42e7ded0",
   "metadata": {},
   "source": [
    "## Criando o index e carregando\n",
    "\n",
    "O método IndexCreator executa a criação da tabela e vincula qual modelo de embeddings será utilizado no índice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd667b39-b0c6-49ce-8c6b-eb6defd1571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_creator = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=Cassandra,\n",
    "    embedding=embedding_generator,\n",
    "    text_splitter=RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=30,\n",
    "    ),\n",
    "    vectorstore_kwargs={\n",
    "        'session': cassio.config.resolve_session(),\n",
    "        'keyspace': 'demo',\n",
    "        'table_name': table_name,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bd88bd-ceb2-4157-888d-c855fd2dc8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os dados de um fundo de investimento específico\n",
    "loader = PyPDFDirectoryLoader('./funds')\n",
    "#loader = PyPDFLoader(\"./funds/RealInvFIM0623.pdf\")\n",
    "index = index_creator.from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa0574-4fe5-4dbe-b46b-aceb152483d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os dados de um fundo de investimento específico\n",
    "#loader = PyPDFDirectoryLoader('./funds')\n",
    "loader = PyPDFLoader(\"./funds/RealInvFIM0623.pdf\")\n",
    "index = index_creator.from_loaders([loader])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b88c22-fcd8-4904-b3ae-419126b9ffdf",
   "metadata": {},
   "source": [
    "### Se o seu índice já estiver carregado...\n",
    "\n",
    "... você pode referenciá-lo desta maneira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fac01f-44ed-414a-8742-f215254b6182",
   "metadata": {},
   "outputs": [],
   "source": [
    "CassVectorStore = Cassandra(\n",
    "    session= cassio.config.resolve_session(),\n",
    "    keyspace= 'demo',\n",
    "    table_name= table_name,\n",
    "    embedding=embedding_generator\n",
    ")\n",
    "\n",
    "index = VectorStoreIndexWrapper(\n",
    "    vectorstore=CassVectorStore\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c291b4-1f4d-4569-a48f-a50030d88266",
   "metadata": {},
   "source": [
    "### Conferindo os dados gravados\n",
    "\n",
    "Faremos uma query no Vector Store para conferir como os dados foram gravados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bcffd5-81de-496b-b181-197c037b4356",
   "metadata": {},
   "outputs": [],
   "source": [
    "cqlSelect = f\"\"\"SELECT * FROM {keyspace}.{table_name} \n",
    "WHERE metadata_s['source'] = './funds/RealInvFIM0623.pdf'\n",
    "LIMIT 3;\"\"\"\n",
    "rows = cassio.config.resolve_session().execute(cqlSelect)\n",
    "for row_i, row in enumerate(rows):\n",
    "    print(f'\\n{\"-\"*50}')\n",
    "    print(f'\\nRow {row_i}:')\n",
    "    print(f'    document_id:      {row.row_id}')\n",
    "    print(f'    embedding_vector: {str(row.vector)[:64]} ...')\n",
    "    print(f'    metadata_blob:    {row.metadata_s}')\n",
    "    print(f'    document:         {row.body_blob} ...')\n",
    "\n",
    "print('\\n...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ade180-aeb1-40e8-b3e9-c0da3b16f29b",
   "metadata": {},
   "source": [
    "# Utilizando o documento para responder uma pergunta\n",
    "\n",
    "O LangChain possui métodos que buscam documentos similares no index e compõem um prompt para gerar a resposta a uma pergunta.\n",
    "\n",
    "Este método são o ``ìndex.query`` e ``index.query_with_sources``.\n",
    "\n",
    "A diferença entre estes métodos é o tipo de RetrievalChain que são utilizados por cada um.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f12a0e-6c63-48b2-8a8a-af4f9361b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.debug = False\n",
    "langchain.verbose = True\n",
    "\n",
    "QUERY = \"Recomende fundos com baixo risco e com retirada em d+1?\"\n",
    "\n",
    "print(\"\\nResposta com o query_with_sources:\")\n",
    "print(index.query_with_sources(question=QUERY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfb9b75-c563-44c4-99c2-e474e1777070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.debug = False\n",
    "langchain.verbose = True\n",
    "\n",
    "QUERY = \"Qual o risco do fundo?\"\n",
    "\n",
    "print(\"Resposta com o query:\")\n",
    "print(index.query(question=QUERY))\n",
    "\n",
    "print(\"\\nResposta com o query_with_sources:\")\n",
    "print(index.query_with_sources(question=QUERY))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266f6df3-b988-4da1-b9d2-fa05d032f881",
   "metadata": {},
   "source": [
    "Estes métodos funcionam bem, mas na vida real precisaremos de mais flexibilidade e detalhamento no comportamento do LLM, modificando o prompt para fornecer mais instruções, ou filtrando documentos que queremos que sejam considerados.\n",
    "\n",
    "Por isso, o melhor é usarmos as mesmas Chains utilizadas acima, mas agora podendo especificar mais detalhes. \n",
    "\n",
    "Vamos ver como fazer isso.\n",
    "\n",
    "## Q/A passo a passo\n",
    "\n",
    "### Buscando documentos pela similaridade\n",
    "\n",
    "A busca pelos documentos que vão preencher o contexto pode utilizar estratégias distintas.\n",
    "\n",
    "Iniciaremos com a busca por similaridade de vetores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a955974-55a7-457c-bbbd-0bee92dc5b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "matchesSim = CassVectorStore.search(QUERY, search_type='similarity', k=4)\n",
    "for i, doc in enumerate(matchesSim):\n",
    "    print(f'[{i:2}]: \"{doc.page_content}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf4bba2-860c-4461-9e02-31e14924c476",
   "metadata": {},
   "source": [
    "## Utilizando a Chain com prompt personalizado\n",
    "Vamos usar estes documentos para preencher o prompt. Note que nos próximos passos iremos importar as Chains e também definir o prompt que queremos utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7f984e-d2ed-4b78-9359-61cdc04aa888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb532949-8c17-49da-a6fe-e7a0d3ffa7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "ALWAYS return a \"SOURCES\" part in your answer. Answer in Portuguese.\n",
    "\n",
    "\n",
    "QUESTION: {question}\n",
    "=========\n",
    "{summaries}\n",
    "=========\n",
    "FINAL ANSWER:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"summaries\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59bc0bb-97e4-456b-8639-6f55f5320ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieverSim = CassVectorStore.as_retriever(\n",
    "    search_type='similarity',\n",
    "    search_kwargs={\n",
    "        'k': 5,\n",
    "        #'filter': {\"source\": \"./funds/RealInvFIM0623.pdf\"}\n",
    "    },\n",
    ")\n",
    "# Create a \"RetrievalQA\" chain\n",
    "chainSim = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retrieverSim,\n",
    "    chain_type_kwargs={\n",
    "        'prompt': PROMPT,\n",
    "        'document_variable_name': 'summaries'\n",
    "    }\n",
    ")\n",
    "# Run it and print results\n",
    "responseSim = chainSim.run(QUERY)\n",
    "print(responseSim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd63e2c-3138-46a6-8d46-0c7ff14eaaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual creation of the \"retriever\" with the 'similarity' search type\n",
    "retrieverMMR = CassVectorStore.as_retriever(\n",
    "    search_type='mmr',\n",
    "    search_kwargs={\n",
    "        'k': 5,\n",
    "        'fetch_k': 10,\n",
    "        'filter': {\"source\": \"./funds/RealInvFIM0623.pdf\"}\n",
    "    },\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Create a \"RetrievalQA\" chain\n",
    "chainMMR = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retrieverMMR,\n",
    "    chain_type_kwargs={\n",
    "        'prompt': PROMPT,\n",
    "        'document_variable_name': 'summaries'\n",
    "    }\n",
    ")\n",
    "# Run it and print results\n",
    "responseSim = chainMMR.run(QUERY)\n",
    "print(responseSim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dead09-73a0-4955-986f-69f90b06f62a",
   "metadata": {},
   "source": [
    "## Adicionando memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192e9b1c-de63-40d7-babf-66f86df50722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import CassandraChatMessageHistory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "memory_table_name = 'astra_agent_memory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93f4f75-0240-4949-9e95-0b8ed862363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_answer(conversation_id, q):\n",
    "    prompt_template = \"\"\"\n",
    "    Given the following extracted parts of a long document and a question, create a final answer in a very short format. \n",
    "    If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "    Answer in Portuguese.\n",
    "\n",
    "\n",
    "    QUESTION: {question}\n",
    "    =========\n",
    "    {summaries}\n",
    "    =========\n",
    "    FINAL ANSWER:\"\"\"\n",
    "\n",
    "    PROMPT = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"summaries\", \"question\"]\n",
    "    )\n",
    "\n",
    "    message_history = CassandraChatMessageHistory(\n",
    "        session_id=conversation_id,\n",
    "        session= cassio.config.resolve_session(),\n",
    "        keyspace= cassio.config.resolve_keyspace(),\n",
    "        ttl_seconds=3600,\n",
    "        table_name=memory_table_name\n",
    "    )\n",
    "\n",
    "    memory = ConversationSummaryBufferMemory(\n",
    "        llm=llm,\n",
    "        chat_memory=message_history,\n",
    "        max_token_limit=50,\n",
    "        buffer=\"\"\n",
    "    )\n",
    "\n",
    "    retrieverSim = CassVectorStore.as_retriever(\n",
    "        search_type='similarity_score_threshold',\n",
    "        search_kwargs={\n",
    "            'k': 5,\n",
    "            'filter': {\"source\": \"./funds/RealInvFIM0623.pdf\"},\n",
    "            \"score_threshold\": .8\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # Create a \"RetrievalQA\" chain\n",
    "    chainSim = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retrieverSim,\n",
    "        memory=memory,\n",
    "        chain_type_kwargs={\n",
    "            'prompt': PROMPT,\n",
    "            'document_variable_name': 'summaries'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Run it and print results\n",
    "    answer = chainSim.run(q)\n",
    "\n",
    "    return answer\n",
    "\n",
    "langchain.debug = True\n",
    "langchain.verbose = False\n",
    "res = get_answer('my_conv2',\"Simule a aplicação de 1000\")\n",
    "print(\"=\"*20)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362eed7a-10ae-44e0-9a61-0fd1ab7ab3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "def get_answer(conversation_id, q):\n",
    "    prompt_template = \"\"\"\n",
    "    Given the following extracted parts of a long document and a question, create a final answer in a very short format. \n",
    "    If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "    Answer in Portuguese.\n",
    "\n",
    "\n",
    "    QUESTION: {question}\n",
    "    =========\n",
    "    Chat History:\n",
    "    ========\n",
    "    {chat_history}\n",
    "    ========\n",
    "    {summaries}\n",
    "    =========\n",
    "    FINAL ANSWER:\"\"\"\n",
    "\n",
    "    PROMPT = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"summaries\", \"question\",\"chat_history\"]\n",
    "    )\n",
    "\n",
    "    message_history = CassandraChatMessageHistory(\n",
    "        session_id=conversation_id,\n",
    "        session= cassio.config.resolve_session(),\n",
    "        keyspace= cassio.config.resolve_keyspace(),\n",
    "        ttl_seconds=3600,\n",
    "        table_name=memory_table_name\n",
    "    )\n",
    "\n",
    "    memory = ConversationSummaryBufferMemory(\n",
    "        llm=llm,\n",
    "        chat_memory=message_history,\n",
    "        max_token_limit=50,\n",
    "        buffer=\"\",\n",
    "        return_messages=True,\n",
    "        memory_key=\"chat_history\"\n",
    "    )\n",
    "\n",
    "    retrieverSim = CassVectorStore.as_retriever(\n",
    "        search_type='similarity_score_threshold',\n",
    "        search_kwargs={\n",
    "            'k': 5,\n",
    "            'filter': {\"source\": \"./funds/RealInvFIM0623.pdf\"},\n",
    "            \"score_threshold\": .8\n",
    "        },\n",
    "    )\n",
    "\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever = retrieverSim,\n",
    "        memory=memory\n",
    "    )\n",
    "\n",
    "    answer = qa({\"question\": q})\n",
    "    \n",
    "    return answer\n",
    "\n",
    "langchain.debug = False\n",
    "langchain.verbose = True\n",
    "res = get_answer('my_conv3',\"Qual o rendimento?\")\n",
    "print(\"=\"*20)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb8f09-1d9d-4377-9e28-8caec2fe1e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = CassandraChatMessageHistory(\n",
    "        session_id='my_conv1',\n",
    "        session= cassio.config.resolve_session(),\n",
    "        keyspace= cassio.config.resolve_keyspace(),\n",
    "        ttl_seconds=3600,\n",
    "        table_name=memory_table_name\n",
    "    )\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "        llm=llm,\n",
    "        chat_memory=message_history,\n",
    "        max_token_limit=50,\n",
    "        buffer=\"\",\n",
    "        return_messages=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b3c409-cc4d-4018-8e08-3504373b78c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
